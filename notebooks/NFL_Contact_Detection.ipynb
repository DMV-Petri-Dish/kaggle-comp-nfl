{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MX_8Ty_GbZFW"
   },
   "outputs": [],
   "source": [
    "# install required libraries here\n",
    "\n",
    "!pip install albumentations av cv2 kaggle fastai fastkaggle pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1678819398397,
     "user": {
      "displayName": "John Kroeker",
      "userId": "17401373758060542200"
     },
     "user_tz": 240
    },
    "id": "ppL2eCsTVCHy"
   },
   "outputs": [],
   "source": [
    "from fastkaggle import *\n",
    "from google.colab import files\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.widgets import *\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8Q3rVWObPA-"
   },
   "outputs": [],
   "source": [
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXaxXLZPbd7W"
   },
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63593,
     "status": "ok",
     "timestamp": 1678819872446,
     "user": {
      "displayName": "John Kroeker",
      "userId": "17401373758060542200"
     },
     "user_tz": 240
    },
    "id": "Tj6496C0cOWV",
    "outputId": "96851be2-ba48-4071-b371-2b55ad9e7716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading nfl-player-contact-detection.zip to /content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.84G/3.84G [00:34<00:00, 119MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "comp = 'nfl-player-contact-detection'\n",
    "\n",
    "path = setup_comp(comp, install='fastai \"timm>=0.6.2.dev0\"')\n",
    "\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "The most important thing for us to do now that we have the data is to make sure we understand what it is we are given!\n",
    "\n",
    "We do this by looking at the data description on the Kaggle site: https://www.kaggle.com/competitions/nfl-player-contact-detection\n",
    "\n",
    "and printing out some of the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Video\n",
    "\n",
    "# define the path to the test video files\n",
    "test_data = os.path.join(path, 'train')\n",
    "\n",
    "# get_files is a fast.ai function\n",
    "video_paths = get_files(test_data, extensions='.mp4')\n",
    "\n",
    "Video(str(video_paths[0]),embed=True,width=320,height=320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three video angles of each play, Sideline, Endzone and All 29\n",
    "\n",
    "There are also csv files:\n",
    "\n",
    "* `train_labels.csv` - These are the labelled contacts for the videos in the /train folder for every player combination. What this means is that for every single frame of video we have an indicator saying whether player 1 has contacted EVERY other player, player 2 has contacted EVERY other player and so on. This is why this file is over 400MB,it is basically a registry of (player_x * num_players) * num_frames + contacted_occured \n",
    "\n",
    "* `train_baseline_helmets.csv` - These are baseline helmet detection and assignment boxes for the training and test set. These are useful when predicting contacts. It provides the bounding boxes for all detected helmets. Not all helmets are detected in every frame.\n",
    "\n",
    "* `train_player_tracking.csv` -  This is 10 Hz tracking data for each player on the field during the provided plays. What this means is that for every 1/10th of a second, we have the location, acceleration and direction of each player. This is useful for numerous reasons, including figuring out exactly how close players are to each other.\n",
    "\n",
    "* `train_video_metadata.csv` - contains timestamps associated with each Sideline and Endzone view for syncing with the player tracking data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Reading the description of the columns in the train_labels.csv, the contact_id column is an amalgamation of several potentially useful data points. \n",
    "\n",
    "Lets do some feature engineering to parse them out into their own columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contact_id(df):\n",
    "    \"\"\"\n",
    "    Splits out contact_id into seperate columns.\n",
    "    \"\"\"\n",
    "    df[\"game_play\"] = df[\"contact_id\"].str[:12]\n",
    "    df[\"step\"] = df[\"contact_id\"].str.split(\"_\").str[-3].astype(\"int\")\n",
    "    df[\"nfl_player_id_1\"] = df[\"contact_id\"].str.split(\"_\").str[-2]\n",
    "    df[\"nfl_player_id_2\"] = df[\"contact_id\"].str.split(\"_\").str[-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = expand_contact_id(pd.read_csv(os.path.join(path, \"train_labels.csv\")))\n",
    "print(\"Number of labelled contacts : \",len(labels))\n",
    "train_tracking = pd.read_csv(os.path.join(path, \"train_player_tracking.csv\"))\n",
    "print(\"Number of tracking records : \",len(train_tracking))\n",
    "train_helmets = pd.read_csv(os.path.join(path, \"train_baseline_helmets.csv\"))\n",
    "print(\"Number of helmet detections : \",len(train_helmets))\n",
    "train_video_metadata = pd.read_csv(os.path.join(path, \"train_video_metadata.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Filtering\n",
    "\n",
    "360 plays is too many to train on. Choose a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays = ['58168_003392']\n",
    "#plays = ['58168_003392', '58172_003247', '58173_003606', '58174_001792', '58176_002844']\n",
    "train_video_metadata = train_video_metadata[train_video_metadata['game_play'].isin(plays)] \n",
    "train_helmets = train_helmets[train_helmets['game_play'].isin(plays)] \n",
    "train_tracking = train_tracking[train_tracking['game_play'].isin(plays)] \n",
    "labels = labels[labels['game_play'].isin(plays)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features\n",
    "\n",
    "Create a feature set by joining data from several CSV files into one dataframe.\n",
    "\n",
    "Join labeled data of whether players are contacting each other or the ground (train_labels.csv) with distance and acceleration tracking data (train_player_tracking.csv).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1678819960402,
     "user": {
      "displayName": "John Kroeker",
      "userId": "17401373758060542200"
     },
     "user_tz": 240
    },
    "id": "LVYdaLb3otrK",
    "outputId": "d4fd980f-c099-42ec-f5b8-a985de98c9b0"
   },
   "outputs": [],
   "source": [
    "def create_features(df, tr_tracking, merge_col=\"step\", use_cols=[\"x_position\", \"y_position\"]):\n",
    "    output_cols = []\n",
    "    df_combo = (\n",
    "      # cast the nfl_player_id_1 column to a string\n",
    "      df.astype({\"nfl_player_id_1\": \"str\"})\n",
    "      # database-style join\n",
    "      .merge(\n",
    "          # define the columns to be merged on the dataframe being merged\n",
    "          tr_tracking.astype({\"nfl_player_id\": \"str\"})[\n",
    "              [\"game_play\", merge_col, \"nfl_player_id\",] + use_cols\n",
    "          ],\n",
    "          # match values in these columns when merging\n",
    "          left_on=[\"game_play\", merge_col, \"nfl_player_id_1\"],\n",
    "          right_on=[\"game_play\", merge_col, \"nfl_player_id\"],\n",
    "          # in the resulting frame, keep only columns from df\n",
    "          how=\"left\"\n",
    "      )\n",
    "      .rename(columns={c: c+\"_1\" for c in use_cols})\n",
    "       # drop the nfl_player_id column\n",
    "      .drop(\"nfl_player_id\", axis=1)\n",
    "      .merge(\n",
    "          # define the columns to be merged on the dataframe being merged\n",
    "          tr_tracking.astype({\"nfl_player_id\": \"str\"})[\n",
    "              [\"game_play\", merge_col, \"nfl_player_id\",] + use_cols\n",
    "          ],\n",
    "          # match values in these columns when merging\n",
    "          left_on=[\"game_play\", merge_col, \"nfl_player_id_2\"],\n",
    "          right_on=[\"game_play\", merge_col, \"nfl_player_id\"],\n",
    "          # in the resulting frame, keep only columns from df\n",
    "          how=\"left\"\n",
    "      )\n",
    "      .rename(columns={c: c+\"_2\" for c in use_cols})\n",
    "      # drop the nfl_player_id column\n",
    "      .drop(\"nfl_player_id\", axis=1)\n",
    "      .sort_values([\"game_play\", merge_col, \"nfl_player_id_1\", \"nfl_player_id_2\"])\n",
    "      .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    output_cols += [c+\"_1\" for c in use_cols]\n",
    "    output_cols += [c+\"_2\" for c in use_cols]\n",
    "\n",
    "    # find the euclidian distance (p-2) between two players\n",
    "    # create a new column called 'distance' with this data\n",
    "    if (\"x_position\" in use_cols) & (\"y_position\" in use_cols):\n",
    "        index = df_combo['x_position_2'].notnull()\n",
    "\n",
    "        distance_arr = np.full(len(index), np.nan)\n",
    "        tmp_distance_arr = np.sqrt(\n",
    "            np.square(df_combo.loc[index, \"x_position_1\"] - df_combo.loc[index, \"x_position_2\"])\n",
    "            + np.square(df_combo.loc[index, \"y_position_1\"]- df_combo.loc[index, \"y_position_2\"])\n",
    "        )\n",
    "\n",
    "        distance_arr[index] = tmp_distance_arr\n",
    "        df_combo['distance'] = distance_arr\n",
    "        output_cols += [\"distance\"]\n",
    "\n",
    "    df_combo['G_flug'] = (df_combo['nfl_player_id_2']==\"G\")\n",
    "    output_cols += [\"G_flug\"]\n",
    "    return df_combo, output_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = [\n",
    "    'x_position', 'y_position', 'speed', 'distance',\n",
    "    'direction', 'orientation', 'acceleration', 'sa'\n",
    "]\n",
    "\n",
    "train, feature_cols = create_features(labels, train_tracking, use_cols=use_cols)\n",
    "\n",
    "train.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more we can do to clean up this data. We are trying to predict player contact. The distance column describes how far a player has traveled since the last time stamp. If that value is large, the player is running freely and not in contact with another player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where distance is greater than two yards\n",
    "train_filtered = train.query('not distance>2').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Kaggle competition page:\n",
    "\"These videos all contain a frame rate of 59.94 HZ. The moment of snap occurs 5 seconds into the video.\"\n",
    "\n",
    "Create a new 'frame' column using this information and the step column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 59.94 is average frame rate, 5*59.94 is because step=0 starts\n",
    "# from 5s not 0s (The data is labelled starting AFTER the snap)\n",
    "# a step is 1/10th of a second\n",
    "\n",
    "train_filtered['frame'] = (train_filtered['step']/10*59.94+5*59.94).astype('int')+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use ffmpeg to convert each video in jpeg images representing each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in tqdm(train_helmets.video.unique()):\n",
    "    if 'Endzone2' not in video:\n",
    "        !ffmpeg -i nfl-player-contact-detection/train/{video} -q:v 2 -f image2 working/{video}_%04d.jpg -hide_banner -loglevel error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup Tables\n",
    "\n",
    "Create two look up tables:\n",
    "1. find all players in a video\n",
    "2. find all jpg frames belonging to a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video2helmets = {}\n",
    "\n",
    "train_helmets_new = train_helmets.set_index('video')\n",
    "for video in tqdm(train_helmets.video.unique()):\n",
    "    video2helmets[video] = train_helmets_new.loc[video].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video2frames = {}\n",
    "\n",
    "for game_play in tqdm(train_video_metadata.game_play.unique()):\n",
    "    for view in ['Endzone', 'Sideline']:\n",
    "        video = game_play + f'_{view}.mp4'\n",
    "        video2frames[video] = max(list(map(lambda x:int(x.split('_')[-1].split('.')[0]), \\\n",
    "                                           glob.glob(f'working/{video}*'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "with help from Albumentations library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an implementation of TTA\n",
    "# https://www.kaggle.com/code/andrewkh/test-time-augmentation-tta-worth-it\n",
    "\n",
    "# The image is flipped, transposed and contrast adjusted\n",
    "train_aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.75),\n",
    "    A.ShiftScaleRotate(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.25),\n",
    "    A.Normalize(mean=[0.], std=[1.])\n",
    "])\n",
    "\n",
    "valid_aug = A.Compose([\n",
    "    A.Normalize(mean=[0.], std=[1.])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 - 2.5D Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We use twist on traditional 2.5D training. 2.5D training involves stacking multiple similar images on top of each other and feeding it to a model. In this way, the model gets a better idea of what it is we are trying to optimize.\n",
    "\n",
    "#### What we are dealing with here are hundreds of videos of American Football plays, where we already have all of the contacts labelled, so we know to a fairly high level of accuracy when contacts occur in the video. Our job is to feed 'what contact between players looks like' into the model.\n",
    "\n",
    "#### What we do is to feed frames into the model which occur 'around the same time' as the point of contact. This means that for each contact that we are training on, we are using quite a few images.\n",
    "\n",
    "#### For every record in train_labels.csv, create a Tensor.\n",
    "\n",
    "#### The below function defines the transformations we must perform to ready all the data into Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFDataset():\n",
    "    def __init__(self, df, aug=train_aug, mode='train'):\n",
    "        self.df = df\n",
    "        self.frame = df.frame.values\n",
    "        self.feature = df[feature_cols].fillna(-1).values\n",
    "        self.players = df[['nfl_player_id_1','nfl_player_id_2']].values\n",
    "        self.game_play = df.game_play.values\n",
    "        self.aug = aug\n",
    "        self.mode = mode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def getitem(self):\n",
    "        # Create a list of sequential numbers from 0 to the number of occurrences in the dataframe\n",
    "        # This represents the frame indexes \n",
    "        allIndexes = np.arange(len(self.df))\n",
    "        \n",
    "        for idx in allIndexes:\n",
    "            \n",
    "            # the window is how far you will look behind and ahead, from the current frame\n",
    "            # to create a frame sequence for one tensor\n",
    "            window = 24\n",
    "            \n",
    "            # Get the first frame of interest\n",
    "            # The snap occurs five seconds into every video.\n",
    "            # Given our frame rate of ___, this means we always start from 300\n",
    "            \n",
    "            # Also to note, we do not have tracking and labeling data for every frame.\n",
    "            # only every 6th frame: Video framerate is 60Hz and tracking/labels at 10Hz\n",
    "            frame = self.frame[idx]\n",
    "            \n",
    "            # When training, randomize the first pulled frame by +/- 6 frames \n",
    "            if self.mode == 'train':\n",
    "                \n",
    "                frame = frame + random.randint(-6, 6)\n",
    "            \n",
    "            # Consider the two players in the contact\n",
    "            players = []\n",
    "            for p in self.players[idx]:\n",
    "                if p == 'G':\n",
    "                    players.append(p)\n",
    "                else:\n",
    "                    players.append(int(p))\n",
    "            \n",
    "            # Process the frame matching the current frame number for both the EndZone and Sideline videos\n",
    "            for view in ['Endzone', 'Sideline']:\n",
    "                video = self.game_play[idx] + f'_{view}.mp4'\n",
    "                \n",
    "                # Get a mapping of all the players in the video\n",
    "                tmp = video2helmets[video]\n",
    "                \n",
    "                # Narrow the mapping to only those frames in the window size of interest\n",
    "                tmp[tmp['frame'].between(frame-window, frame+window)]\n",
    "                \n",
    "                # Further narrow to frames containing players of interest \n",
    "                tmp = tmp[tmp.nfl_player_id.isin(players)]\n",
    "                \n",
    "                tmp_frames = tmp.frame.values\n",
    "                \n",
    "                # Group the remaining frames and get the mean value of the bounding box dimensions\n",
    "                tmp = tmp.groupby('frame')[['left','width','top','height']].mean()\n",
    "                \n",
    "                # Iterate thru every frame in our window\n",
    "                bboxes = []\n",
    "                for f in range(frame-window, frame+window+1, 1):\n",
    "                    if f in tmp_frames:\n",
    "                        x, w, y, h = tmp.loc[f][['left','width','top','height']]\n",
    "                        bboxes.append([x, w, y, h])\n",
    "                    else:\n",
    "                        bboxes.append([np.nan, np.nan, np.nan, np.nan])\n",
    "                        \n",
    "                # Interpolate bounding boxes for frames where we did not find bboxes\n",
    "                bboxes = pd.DataFrame(bboxes).interpolate(limit_direction='both').values\n",
    "                \n",
    "                # Sample every fourth bounding box\n",
    "                bboxes = bboxes[::4]\n",
    "                \n",
    "                if bboxes.sum() > 0:\n",
    "                    flag = 1\n",
    "                else:\n",
    "                    flag = 0\n",
    "                    \n",
    "                frame_sampled = False\n",
    "                \n",
    "                for i, f in enumerate(range(frame-window, frame+window+1, 4)):\n",
    "                    \n",
    "                    img_new = np.zeros((256, 256), dtype=np.float32)\n",
    "                    \n",
    "                    # Read the frame if it had a valid player 1 helmet bounding box\n",
    "                    if flag == 1 and f <= video2frames[video]:\n",
    "                        \n",
    "                        img = cv2.imread(f'{CFG.ffmpeg_output}/{video}_{f:04d}.jpg', 0)\n",
    "                        \n",
    "                        # Get the bbox\n",
    "                        # Use it to create a new image\n",
    "                        # by zooming in on the two players in the video\n",
    "                        x, w, y, h = bboxes[i]\n",
    "                        \n",
    "                        img = img[int(y+h/2)-128:int(y+h/2)+128,int(x+w/2)-128:int(x+w/2)+128].copy()\n",
    "                        \n",
    "                        img_new[:img.shape[0], :img.shape[1]] = img\n",
    "                        \n",
    "                    imgs.append(img_new)\n",
    "                    \n",
    "            # Cast all features for this record to float\n",
    "            feature = np.float32(self.feature[idx])\n",
    "            \n",
    "            # Transpose the image list and store it\n",
    "            img = np.array(imgs).transpose(1, 2, 0)\n",
    "            \n",
    "            img = self.aug(image=img)[\"image\"]\n",
    "            record_label = np.float32(self.df.contact.values[idx])\n",
    "            \n",
    "            yield (img, feature), record_label"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOf3dVvAzRBib3hg52fLRLg",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
